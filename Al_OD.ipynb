{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Activ Learning Scenario<h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Import libs</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import absolute_import\n",
    "import random\n",
    "import pprint\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import errno\n",
    "import ntpath\n",
    "from shutil import copyfile,move\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.objectives import categorical_crossentropy\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.utils import generic_utils\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras import initializers, regularizers\n",
    "from keras.layers import Input\n",
    "import time\n",
    "sys.path.insert(0,'/home/kamgo/midras/keras-frcnn/keras_frcnn')\n",
    "from keras_frcnn import config\n",
    "from keras_frcnn import roi_helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> unterschiedlische Pahtfolder eingeben<h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathtoTrain ='/home/kamgo/midras/keras-frcnn/train_images' # annotierte Dateien:Seed\n",
    "pathtoTest = '/home/kamgo/midras/keras-frcnn/test_images' # te\n",
    "pathtoImage ='/home/kamgo/midras/data' # alle Bilder und Annotation\n",
    "imgtype = 'jpg'\n",
    "anotationtype='xml'\n",
    "Batchsize = 10 # denn ich nutzt die \"pool_Based sampling mit N  size\"\n",
    "Seed =[]\n",
    "Unlabeliert = []\n",
    "pathConfig ='/home/kamgo/midras/keras-frcnn/config.pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Auswahl von n-näschten Bilder</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Diese Funktion wird labellierte Data auswählen. es wurde dafür einen Batch eingestellt,\n",
    "    damit nicht alle Data im Seed ausgewählt seien sonder einen Menge (Batch size).\"\"\"\n",
    "    # Eingabe: Path to Seed, Batch size\n",
    "    # Verarbeitung:Es wurde datei in train Verzeichnis des Models (Model/research/object_detection/images/train)\n",
    "    # Ausgabe: cvs :datei für tensorflow api\n",
    "\n",
    "pathImg = pathtoImage + str('/*.') + imgtype\n",
    "pathAnotattion = pathtoImage + str('/*.')+ anotationtype \n",
    "imgFiles = glob.glob(pathImg)\n",
    "anotationFiles = glob.glob(pathAnotattion)\n",
    "count = 0\n",
    "for img in imgFiles:\n",
    "    for anot in anotationFiles:\n",
    "        if (ntpath.basename(os.path.splitext(anot)[0])== ntpath.basename(os.path.splitext(img)[0])):\n",
    "            if count<Batchsize-1:\n",
    "                Seed.append(img)\n",
    "                Seed.append(anot)\n",
    "                count = count + 1\n",
    "            else:\n",
    "                Unlabeliert.append(img)\n",
    "                Unlabeliert.append(anot)\n",
    "\n",
    "for data in Seed:\n",
    "    filename= os.path.basename(data)\n",
    "    copyfile(data,os.path.join(pathtoTrain,filename))\n",
    "\n",
    "for data in Unlabeliert:\n",
    "    filename= os.path.basename(data)\n",
    "    copyfile(data,os.path.join(pathtoTest,filename))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Erstellung von .txt file für die trainingsphase</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_to_csv(pathtoTrain):\n",
    "    xml_list = []\n",
    "    for xml_file in glob.glob(pathtoTrain + '/*.xml'):\n",
    "        tree = ET.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "        for member in root.findall('object'):\n",
    "            value = (root.find('path').text,                     \n",
    "                     int(member[4][0].text),\n",
    "                     int(member[4][1].text),\n",
    "                     int(member[4][2].text),\n",
    "\t\t     int(member[4][3].text),\n",
    "                     member[0].text\n",
    "                     )\n",
    "            xml_list.append(value)\n",
    "    column_name = ['path', 'xmin', 'ymin', 'xmax', 'ymax','class']\n",
    "    xml_df = pd.DataFrame(xml_list,columns=column_name)\n",
    "    \n",
    "    train_images =os.path.basename(os.path.normpath(pathtoTrain))\n",
    "    test_images = os.path.basename(os.path.normpath(pathtotrain))\n",
    "    \n",
    "    for folder in [train_images,test_images]:\n",
    "        image_path = os.path.join(os.getcwd() + '../', (folder))\n",
    "        xml_df.to_csv((os.getcwd() +'/'+ folder + '_labels.txt'), index=None, header=False)\n",
    "        print('Successfully converted xml to txt.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Trainingsphase</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "Parsing annotation files\n",
      "shape Error!\n",
      "Traceback (most recent call last):\n",
      "  File \"train_frcnn.py\", line 79, in <module>\n",
      "    all_imgs, classes_count, class_mapping = get_data(options.train_path)\n",
      "  File \"/home/kamgo/midras/keras-frcnn/keras_frcnn/simple_parser.py\", line 44, in get_data\n",
      "    all_imgs[filename]['width'] = cols\n",
      "UnboundLocalError: local variable 'cols' referenced before assignment\n"
     ]
    }
   ],
   "source": [
    "!python train_frcnn.py -o simple --num_epochs 10 -p  train_images_labels.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Einsetzung des Models</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Drohne', 1: 'bg'}\n",
      "WARNING:tensorflow:From /home/kamgo/anaconda3/envs/midras/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Loading weights from ./model_frcnn.hdf5\n",
      "20190221-162426275388.jpg\n",
      "20190221-162430312549.jpg\n",
      "20190221-162432336270.jpg\n",
      "20190221-162433345146.jpg\n",
      "20190221-162434363145.jpg\n",
      "20190221-162436385001.jpg\n",
      "20190221-162437396203.jpg\n",
      "20190221-162438407956.jpg\n",
      "20190221-162440434672.jpg\n",
      "20190221-162441445685.jpg\n",
      "20190221-162442458133.jpg\n",
      "20190221-162444482494.jpg\n",
      "20190221-162446506758.jpg\n",
      "20190221-162446527569.jpg\n",
      "20190221-162447515964.jpg\n",
      "20190221-162449547891.jpg\n",
      "20190221-162452588341.jpg\n",
      "20190221-162453603774.jpg\n",
      "20190221-162454619094.jpg\n",
      "20190221-162456646501.jpg\n",
      "20190221-162457666735.jpg\n",
      "20190221-162458671140.jpg\n",
      "20190221-162500691235.jpg\n",
      "20190221-162511859677.jpg\n",
      "20190221-162513884633.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a3e48ebe1426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;31m# get the feature maps and output from the RPN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mY1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_rpn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1835\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/envs/midras/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1329\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                     \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midras/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/midras/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sys.setrecursionlimit(40000)\n",
    "\n",
    "with open(pathConfig, 'rb') as f_in:\n",
    "\tC = pickle.load(f_in)\n",
    "\n",
    "if C.network == 'resnet50':\n",
    "\timport keras_frcnn.resnet as nn\n",
    "elif C.network == 'vgg':\n",
    "\timport keras_frcnn.vgg as nn\n",
    "\n",
    "# turn off any data augmentation at test time\n",
    "C.use_horizontal_flips = False\n",
    "C.use_vertical_flips = False\n",
    "C.rot_90 = False\n",
    "\n",
    "img_path = pathtoTest  # filetest\n",
    "\n",
    "def format_img_size(img, C):\n",
    "\t\"\"\" formats the image size based on config \"\"\"\n",
    "\timg_min_side = float(C.im_size)\n",
    "\t(height,width,_) = img.shape\n",
    "\t\t\n",
    "\tif width <= height:\n",
    "\t\tratio = img_min_side/width\n",
    "\t\tnew_height = int(ratio * height)\n",
    "\t\tnew_width = int(img_min_side)\n",
    "\telse:\n",
    "\t\tratio = img_min_side/height\n",
    "\t\tnew_width = int(ratio * width)\n",
    "\t\tnew_height = int(img_min_side)\n",
    "\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\treturn img, ratio\t\n",
    "\n",
    "def format_img_channels(img, C):\n",
    "\t\"\"\" formats the image channels based on config \"\"\"\n",
    "\timg = img[:, :, (2, 1, 0)]\n",
    "\timg = img.astype(np.float32)\n",
    "\timg[:, :, 0] -= C.img_channel_mean[0]\n",
    "\timg[:, :, 1] -= C.img_channel_mean[1]\n",
    "\timg[:, :, 2] -= C.img_channel_mean[2]\n",
    "\timg /= C.img_scaling_factor\n",
    "\timg = np.transpose(img, (2, 0, 1))\n",
    "\timg = np.expand_dims(img, axis=0)\n",
    "\treturn img\n",
    "\n",
    "def format_img(img, C):\n",
    "\t\"\"\" formats an image for model prediction based on config \"\"\"\n",
    "\timg, ratio = format_img_size(img, C)\n",
    "\timg = format_img_channels(img, C)\n",
    "\treturn img, ratio\n",
    "\n",
    "# Method to transform the coordinates of the bounding box to its original size\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "\n",
    "\treal_x1 = int(round(x1 // ratio))\n",
    "\treal_y1 = int(round(y1 // ratio))\n",
    "\treal_x2 = int(round(x2 // ratio))\n",
    "\treal_y2 = int(round(y2 // ratio))\n",
    "\n",
    "\treturn (real_x1, real_y1, real_x2 ,real_y2)\n",
    "\n",
    "class_mapping = C.class_mapping\n",
    "\n",
    "if 'bg' not in class_mapping:\n",
    "\tclass_mapping['bg'] = len(class_mapping)\n",
    "\n",
    "class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "print(class_mapping)\n",
    "class_to_color = {class_mapping[v]: np.random.randint(0, 255, 3) for v in class_mapping}\n",
    "C.num_rois = int(32)\n",
    "\n",
    "if C.network == 'resnet50':\n",
    "\tnum_features = 1024\n",
    "elif C.network == 'vgg':\n",
    "\tnum_features = 512\n",
    "\n",
    "if K.image_dim_ordering() == 'th':\n",
    "\tinput_shape_img = (3, None, None)\n",
    "\tinput_shape_features = (num_features, None, None)\n",
    "else:\n",
    "\tinput_shape_img = (None, None, 3)\n",
    "\tinput_shape_features = (None, None, num_features)\n",
    "\n",
    "\n",
    "img_input = Input(shape=input_shape_img)\n",
    "roi_input = Input(shape=(C.num_rois, 4))\n",
    "feature_map_input = Input(shape=input_shape_features)\n",
    "\n",
    "# define the base network (resnet here, can be VGG, Inception, etc)\n",
    "shared_layers = nn.nn_base(img_input, trainable=True)\n",
    "\n",
    "# define the RPN, built on the base layers\n",
    "num_anchors = len(C.anchor_box_scales) * len(C.anchor_box_ratios)\n",
    "rpn_layers = nn.rpn(shared_layers, num_anchors)\n",
    "\n",
    "classifier = nn.classifier(feature_map_input, roi_input, C.num_rois, nb_classes=len(class_mapping), trainable=True)\n",
    "\n",
    "model_rpn = Model(img_input, rpn_layers)\n",
    "model_classifier_only = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "model_classifier = Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "print('Loading weights from {}'.format(C.model_path))\n",
    "model_rpn.load_weights(C.model_path, by_name=True)\n",
    "model_classifier.load_weights(C.model_path, by_name=True)\n",
    "\n",
    "model_rpn.compile(optimizer='sgd', loss='mse')\n",
    "model_classifier.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "all_imgs = []\n",
    "\n",
    "classes = {}\n",
    "\n",
    "bbox_threshold = 0.8\n",
    "\n",
    "visualise = True\n",
    "\n",
    "for idx, img_name in enumerate(sorted(os.listdir(img_path))):\n",
    "\tif not img_name.lower().endswith(('.bmp', '.jpeg', '.jpg', '.png', '.tif', '.tiff')):\n",
    "\t\tcontinue\n",
    "\tprint(img_name)\n",
    "\tst = time.time()\n",
    "\tfilepath = os.path.join(img_path,img_name)\n",
    "\n",
    "\timg = cv2.imread(filepath)\n",
    "\n",
    "\tX, ratio = format_img(img, C)\n",
    "\n",
    "\tif K.image_dim_ordering() == 'tf':\n",
    "\t\tX = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "\t# get the feature maps and output from the RPN\n",
    "\t[Y1, Y2, F] = model_rpn.predict(X)\n",
    "    \n",
    "--num_epochs\n",
    "--num_epochs\n",
    "--num_epochsto_roi(Y1, Y2, C, K.image_dim_ordering(), overlap_thresh=0.7)\n",
    "--num_epochs\n",
    "# convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "R[:, 2] -= R[:, 0]\n",
    "R[:, 3] -= R[:, 1]\n",
    "\n",
    "# apply the spatial pyramid pooling to the proposed regions\n",
    "bboxes = {}\n",
    "probs = {}\n",
    "\n",
    "for jk in range(R.shape[0]//C.num_rois + 1):\n",
    "    ROIs = np.expand_dims(R[C.num_rois*jk:C.num_rois*(jk+1), :], axis=0)\n",
    "    if ROIs.shape[1] == 0:\n",
    "        break\n",
    "\n",
    "    if jk == R.shape[0]//C.num_rois:\n",
    "        #pad R\n",
    "        curr_shape = ROIs.shape\n",
    "        target_shape = (curr_shape[0],C.num_rois,curr_shape[2])\n",
    "        ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "        ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "        ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "        ROIs = ROIs_padded\n",
    "\n",
    "    [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "    for ii in range(P_cls.shape[1]):\n",
    "\n",
    "        if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "            continue\n",
    "\n",
    "        cls_name = class_mapping[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "        if cls_name not in bboxes:\n",
    "            bboxes[cls_name] = []\n",
    "            probs[cls_name] = []\n",
    "\n",
    "        (x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "        cls_num = np.argmax(P_cls[0, ii, :])\n",
    "        try:\n",
    "            (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "            tx /= C.classifier_regr_std[0]\n",
    "            ty /= C.classifier_regr_std[1]\n",
    "            tw /= C.classifier_regr_std[2]\n",
    "            th /= C.classifier_regr_std[3]\n",
    "            x, y, w, h = roi_helpers.apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "        except:\n",
    "            pass\n",
    "        bboxes[cls_name].append([C.rpn_stride*x, C.rpn_stride*y, C.rpn_stride*(x+w), C.rpn_stride*(y+h)])\n",
    "        probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "all_dets = []\n",
    "\n",
    "for key in bboxes:\n",
    "    bbox = np.array(bboxes[key])\n",
    "\n",
    "    new_boxes, new_probs = roi_helpers.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.5)\n",
    "    for jk in range(new_boxes.shape[0]):\n",
    "        (x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "\n",
    "        (real_x1, real_y1, real_x2, real_y2) = get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "\n",
    "        cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),2)\n",
    "\n",
    "        textLabel = '{}: {}'.format(key,int(100*new_probs[jk]))\n",
    "        all_dets.append((key,100*new_probs[jk]))\n",
    "\n",
    "        (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
    "        textOrg = (real_x1, real_y1-0)\n",
    "\n",
    "        cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 2)\n",
    "        cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "        cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "\n",
    "print('Elapsed time = {}'.format(time.time() - st))\n",
    "print(all_dets)\n",
    "cv2.imshow('img', img)\n",
    "cv2.waitKey(0)\n",
    "# cv2.imwrite('./results_imgs/{}.png'.format(idx),img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
